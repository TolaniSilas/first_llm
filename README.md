# Simple FastAPI for Q&A LLM System
My First LLM System 


## Project Description

This project is a simple FastAPI application designed to provide a Question and Answer (Q&A) system using a Large Language Model (LLM). The system allows users to submit questions and receive answers generated by the LLM. It showcases how to integrate FastAPI with an LLM for creating interactive and intelligent applications.


## Features

- RESTful API endpoints for submitting questions and receiving answers.
- Integration with a Large Language Model for generating responses.
- FastAPI-based backend for efficient and scalable interaction.


## Installation

Follow these steps to set up the project on your local machine:

  1. **Clone the Repository:**
     ```
     git clone https://github.com/TolaniSilas/first_llm.git
     ```
  2. **Navigate to the Project Directory:**
     ```
     cd first_llm
     ```
  3. **Create and Activate a Virtual Environment:**
     ```
     python -m venv venv
     source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
     ```
  4. **Install Dependencies:**
     ```
     pip install -r requirements.txt
     ```


## Usage

1. **Run the FastAPI Application:**
   ```
   uvicorn app:app --reload
   ```
2. **For testing in Postman**

  **Submit a Question:**
   **Endpoint:** `POST /chat`

   **Request Body:**
   ```json
   {
    "question": "What is the capital of France?"
    "model": "llama-3.1-8b-instant",
    "temperature": 0.2
   }
   ```


## Code Explanation

- **`app.py`**: Contains the FastAPI application setup and route definitions. The `/chat` endpoint handles incoming questions and interacts with the LLM to generate answers.

- **`llm.py`**: Manages the interaction with the Large Language Model. It includes functions for sending questions to the LLM and receiving responses.

- **`.env`**: Stores the environment variables such as API keys(Groq Api Keys). Ensure you have set up this file with the correct values before running the application.


## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


